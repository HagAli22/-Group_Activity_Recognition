2025-10-29 10:03:04 - root - INFO - Resume Baseline_B8 Training
2025-10-29 10:03:04 - root - INFO - Training videos: 24 videos
2025-10-29 10:03:04 - root - INFO - Validation videos: 15 videos
2025-10-29 10:03:04 - root - INFO - Training video IDs: [1, 3, 6, 7, 10, 13, 15, 16, 18, 22, 23, 31, 32, 36, 38, 39, 40, 41, 42, 48, 50, 52, 53, 54]
2025-10-29 10:03:04 - root - INFO - Validation video IDs: [0, 2, 8, 12, 17, 19, 24, 26, 27, 28, 30, 33, 46, 49, 51]
2025-10-29 10:03:04 - root - INFO - Using device: cuda
2025-10-29 10:03:16 - root - INFO - Training dataset size: 2152
2025-10-29 10:03:16 - root - INFO - Validation dataset size: 1341
2025-10-29 10:03:16 - root - INFO - Checkpoint directory created: Baseline_B8/checkpoints
2025-10-29 10:03:16 - root - INFO - Starting training for 40 epochs
2025-10-29 10:03:16 - root - INFO - Starting epoch 24/40
2025-10-29 10:04:41 - root - INFO -   Batch 50/269 - Loss: 0.7408, Acc: 96.50%
2025-10-29 10:06:04 - root - INFO -   Batch 100/269 - Loss: 0.7501, Acc: 97.00%
2025-10-29 10:07:27 - root - INFO -   Batch 150/269 - Loss: 0.9990, Acc: 97.33%
2025-10-29 10:08:48 - root - INFO -   Batch 200/269 - Loss: 0.7617, Acc: 97.12%
2025-10-29 10:10:08 - root - INFO -   Batch 250/269 - Loss: 0.7293, Acc: 97.20%
2025-10-29 10:10:38 - root - INFO - Running validation...
2025-10-29 10:14:39 - root - INFO - Learning rate reduced from 0.000100 to 0.000091
2025-10-29 10:14:39 - root - INFO - Epoch 24/40 Results:
2025-10-29 10:14:39 - root - INFO -   Train Loss: 0.7908, Train Acc: 97.12%
2025-10-29 10:14:39 - root - INFO -   Val Loss: 0.9616, Val Acc: 87.70%
2025-10-29 10:14:39 - root - INFO -   Learning Rate: 0.000091
2025-10-29 10:14:39 - root - INFO - ------------------------------------------------------------
2025-10-29 10:14:39 - root - INFO - Starting epoch 25/40
2025-10-29 10:16:03 - root - INFO -   Batch 50/269 - Loss: 0.7146, Acc: 97.75%
2025-10-29 10:17:20 - root - INFO -   Batch 100/269 - Loss: 0.7615, Acc: 97.50%
2025-10-29 10:18:40 - root - INFO -   Batch 150/269 - Loss: 0.7314, Acc: 97.33%
2025-10-29 10:19:59 - root - INFO -   Batch 200/269 - Loss: 0.7201, Acc: 97.25%
2025-10-29 10:21:18 - root - INFO -   Batch 250/269 - Loss: 0.6876, Acc: 97.45%
2025-10-29 10:21:46 - root - INFO - Running validation...
2025-10-29 10:25:31 - root - INFO - Learning rate reduced from 0.000091 to 0.000091
2025-10-29 10:25:31 - root - INFO - Epoch 25/40 Results:
2025-10-29 10:25:31 - root - INFO -   Train Loss: 0.7846, Train Acc: 97.49%
2025-10-29 10:25:31 - root - INFO -   Val Loss: 0.9651, Val Acc: 87.77%
2025-10-29 10:25:31 - root - INFO -   Learning Rate: 0.000091
2025-10-29 10:25:31 - root - INFO - ------------------------------------------------------------
2025-10-29 10:25:31 - root - INFO - Starting epoch 26/40
2025-10-29 10:26:56 - root - INFO -   Batch 50/269 - Loss: 0.7118, Acc: 97.25%
2025-10-29 10:28:14 - root - INFO -   Batch 100/269 - Loss: 0.7711, Acc: 97.25%
2025-10-29 10:29:35 - root - INFO -   Batch 150/269 - Loss: 0.7539, Acc: 97.75%
2025-10-29 10:30:52 - root - INFO -   Batch 200/269 - Loss: 0.6976, Acc: 97.75%
2025-10-29 10:32:10 - root - INFO -   Batch 250/269 - Loss: 0.7867, Acc: 97.70%
2025-10-29 10:32:38 - root - INFO - Running validation...
2025-10-29 10:36:20 - root - INFO - Learning rate reduced from 0.000091 to 0.000091
2025-10-29 10:36:20 - root - INFO - Epoch 26/40 Results:
2025-10-29 10:36:20 - root - INFO -   Train Loss: 0.7831, Train Acc: 97.54%
2025-10-29 10:36:20 - root - INFO -   Val Loss: 0.9663, Val Acc: 87.99%
2025-10-29 10:36:20 - root - INFO -   Learning Rate: 0.000091
2025-10-29 10:36:20 - root - INFO - ------------------------------------------------------------
2025-10-29 10:36:20 - root - INFO - Starting epoch 27/40
2025-10-29 10:37:44 - root - INFO -   Batch 50/269 - Loss: 0.7122, Acc: 96.75%
2025-10-29 10:39:01 - root - INFO -   Batch 100/269 - Loss: 1.2893, Acc: 98.12%
2025-10-29 10:40:23 - root - INFO -   Batch 150/269 - Loss: 0.7205, Acc: 98.25%
2025-10-29 10:41:40 - root - INFO -   Batch 200/269 - Loss: 0.7727, Acc: 98.31%
2025-10-29 10:43:02 - root - INFO -   Batch 250/269 - Loss: 0.6877, Acc: 98.45%
2025-10-29 10:43:31 - root - INFO - Running validation...
2025-10-29 10:47:17 - root - INFO - Learning rate reduced from 0.000091 to 0.000091
2025-10-29 10:47:17 - root - INFO - Epoch 27/40 Results:
2025-10-29 10:47:17 - root - INFO -   Train Loss: 0.7616, Train Acc: 98.37%
2025-10-29 10:47:17 - root - INFO -   Val Loss: 0.9511, Val Acc: 88.52%
2025-10-29 10:47:17 - root - INFO -   Learning Rate: 0.000091
2025-10-29 10:47:17 - root - INFO - ------------------------------------------------------------
2025-10-29 10:47:17 - root - INFO - New best model saved with validation loss: 0.9511
2025-10-29 10:47:17 - root - INFO - Best model saved to: best_Baseline_B8_model.pth
2025-10-29 10:47:17 - root - INFO - Starting epoch 28/40
2025-10-29 10:48:41 - root - INFO -   Batch 50/269 - Loss: 0.7753, Acc: 98.75%
2025-10-29 10:49:59 - root - INFO -   Batch 100/269 - Loss: 0.6902, Acc: 98.50%
2025-10-29 10:51:17 - root - INFO -   Batch 150/269 - Loss: 0.7323, Acc: 98.42%
2025-10-29 10:52:33 - root - INFO -   Batch 200/269 - Loss: 0.6971, Acc: 98.00%
2025-10-29 10:53:53 - root - INFO -   Batch 250/269 - Loss: 0.7484, Acc: 98.05%
2025-10-29 10:54:21 - root - INFO - Running validation...
2025-10-29 10:58:03 - root - INFO - Learning rate reduced from 0.000091 to 0.000091
2025-10-29 10:58:03 - root - INFO - Epoch 28/40 Results:
2025-10-29 10:58:03 - root - INFO -   Train Loss: 0.7676, Train Acc: 98.05%
2025-10-29 10:58:03 - root - INFO -   Val Loss: 0.9624, Val Acc: 88.37%
2025-10-29 10:58:03 - root - INFO -   Learning Rate: 0.000091
2025-10-29 10:58:03 - root - INFO - ------------------------------------------------------------
2025-10-29 10:58:03 - root - INFO - Starting epoch 29/40
2025-10-29 10:59:28 - root - INFO -   Batch 50/269 - Loss: 0.7404, Acc: 98.25%
2025-10-29 11:00:48 - root - INFO -   Batch 100/269 - Loss: 0.6932, Acc: 98.50%
2025-10-29 11:02:07 - root - INFO -   Batch 150/269 - Loss: 0.8616, Acc: 98.75%
2025-10-29 11:03:24 - root - INFO -   Batch 200/269 - Loss: 0.7064, Acc: 98.56%
2025-10-29 11:04:44 - root - INFO -   Batch 250/269 - Loss: 0.7568, Acc: 98.30%
2025-10-29 11:05:12 - root - INFO - Running validation...
2025-10-29 11:08:58 - root - INFO - Learning rate reduced from 0.000091 to 0.000091
2025-10-29 11:08:58 - root - INFO - Epoch 29/40 Results:
2025-10-29 11:08:58 - root - INFO -   Train Loss: 0.7640, Train Acc: 98.33%
2025-10-29 11:08:58 - root - INFO -   Val Loss: 0.9735, Val Acc: 87.40%
2025-10-29 11:08:58 - root - INFO -   Learning Rate: 0.000091
2025-10-29 11:08:58 - root - INFO - ------------------------------------------------------------
2025-10-29 11:08:58 - root - INFO - Starting epoch 30/40
2025-10-29 11:10:22 - root - INFO -   Batch 50/269 - Loss: 1.0979, Acc: 98.75%
2025-10-29 11:11:39 - root - INFO -   Batch 100/269 - Loss: 0.6818, Acc: 98.75%
2025-10-29 11:12:58 - root - INFO -   Batch 150/269 - Loss: 0.7153, Acc: 98.75%
2025-10-29 11:14:15 - root - INFO -   Batch 200/269 - Loss: 0.7791, Acc: 98.88%
2025-10-29 11:15:37 - root - INFO -   Batch 250/269 - Loss: 0.6913, Acc: 98.90%
2025-10-29 11:16:05 - root - INFO - Running validation...
2025-10-29 11:19:55 - root - INFO - Learning rate reduced from 0.000091 to 0.000091
2025-10-29 11:19:55 - root - INFO - Epoch 30/40 Results:
2025-10-29 11:19:55 - root - INFO -   Train Loss: 0.7490, Train Acc: 98.93%
2025-10-29 11:19:55 - root - INFO -   Val Loss: 0.9768, Val Acc: 87.47%
2025-10-29 11:19:55 - root - INFO -   Learning Rate: 0.000091
2025-10-29 11:19:55 - root - INFO - ------------------------------------------------------------
2025-10-29 11:19:55 - root - INFO - Starting epoch 31/40
2025-10-29 11:21:21 - root - INFO -   Batch 50/269 - Loss: 0.7167, Acc: 99.00%
2025-10-29 11:22:38 - root - INFO -   Batch 100/269 - Loss: 0.7975, Acc: 98.00%
2025-10-29 11:23:58 - root - INFO -   Batch 150/269 - Loss: 0.7702, Acc: 97.92%
2025-10-29 11:25:15 - root - INFO -   Batch 200/269 - Loss: 0.8069, Acc: 98.06%
2025-10-29 11:26:32 - root - INFO -   Batch 250/269 - Loss: 0.7061, Acc: 98.15%
2025-10-29 11:27:00 - root - INFO - Running validation...
2025-10-29 11:30:44 - root - INFO - Learning rate reduced from 0.000091 to 0.000091
2025-10-29 11:30:44 - root - INFO - Checkpoint saved for epoch 31 at Baseline_B8/checkpoints/checkpoint_epoch_31.pth
2025-10-29 11:30:44 - root - INFO - Epoch 31/40 Results:
2025-10-29 11:30:44 - root - INFO -   Train Loss: 0.7667, Train Acc: 98.09%
2025-10-29 11:30:44 - root - INFO -   Val Loss: 0.9660, Val Acc: 88.14%
2025-10-29 11:30:44 - root - INFO -   Learning Rate: 0.000091
2025-10-29 11:30:44 - root - INFO - ------------------------------------------------------------
2025-10-29 11:30:44 - root - INFO - Starting epoch 32/40
2025-10-29 11:32:07 - root - INFO -   Batch 50/269 - Loss: 0.6802, Acc: 99.50%
2025-10-29 11:33:22 - root - INFO -   Batch 100/269 - Loss: 0.7106, Acc: 98.62%
2025-10-29 11:34:39 - root - INFO -   Batch 150/269 - Loss: 0.6923, Acc: 98.58%
2025-10-29 11:35:54 - root - INFO -   Batch 200/269 - Loss: 0.7806, Acc: 98.56%
2025-10-29 11:37:12 - root - INFO -   Batch 250/269 - Loss: 0.6908, Acc: 98.65%
2025-10-29 11:37:39 - root - INFO - Running validation...
2025-10-29 11:41:20 - root - INFO - Learning rate reduced from 0.000091 to 0.000091
2025-10-29 11:41:20 - root - INFO - Checkpoint saved for epoch 32 at Baseline_B8/checkpoints/checkpoint_epoch_32.pth
2025-10-29 11:41:20 - root - INFO - Epoch 32/40 Results:
2025-10-29 11:41:20 - root - INFO -   Train Loss: 0.7491, Train Acc: 98.70%
2025-10-29 11:41:20 - root - INFO -   Val Loss: 0.9639, Val Acc: 88.29%
2025-10-29 11:41:20 - root - INFO -   Learning Rate: 0.000091
2025-10-29 11:41:20 - root - INFO - ------------------------------------------------------------
2025-10-29 11:41:20 - root - INFO - Starting epoch 33/40
2025-10-29 11:42:42 - root - INFO -   Batch 50/269 - Loss: 0.6935, Acc: 99.25%
2025-10-29 11:43:59 - root - INFO -   Batch 100/269 - Loss: 1.0226, Acc: 98.88%
2025-10-29 11:45:19 - root - INFO -   Batch 150/269 - Loss: 0.7167, Acc: 98.67%
2025-10-29 11:46:35 - root - INFO -   Batch 200/269 - Loss: 0.6944, Acc: 98.19%
2025-10-29 11:47:53 - root - INFO -   Batch 250/269 - Loss: 0.6946, Acc: 98.25%
2025-10-29 11:48:22 - root - INFO - Running validation...
2025-10-29 11:52:08 - root - INFO - Learning rate reduced from 0.000091 to 0.000091
2025-10-29 11:52:09 - root - INFO - Checkpoint saved for epoch 33 at Baseline_B8/checkpoints/checkpoint_epoch_33.pth
2025-10-29 11:52:09 - root - INFO - Epoch 33/40 Results:
2025-10-29 11:52:09 - root - INFO -   Train Loss: 0.7618, Train Acc: 98.28%
2025-10-29 11:52:09 - root - INFO -   Val Loss: 0.9655, Val Acc: 87.84%
2025-10-29 11:52:09 - root - INFO -   Learning Rate: 0.000091
2025-10-29 11:52:09 - root - INFO - ------------------------------------------------------------
2025-10-29 11:52:09 - root - INFO - Starting epoch 34/40
2025-10-29 11:53:33 - root - INFO -   Batch 50/269 - Loss: 0.9758, Acc: 98.00%
2025-10-29 11:54:51 - root - INFO -   Batch 100/269 - Loss: 0.7088, Acc: 97.75%
2025-10-29 11:56:11 - root - INFO -   Batch 150/269 - Loss: 0.8337, Acc: 98.08%
2025-10-29 11:57:28 - root - INFO -   Batch 200/269 - Loss: 0.7131, Acc: 98.31%
2025-10-29 11:58:47 - root - INFO -   Batch 250/269 - Loss: 0.7178, Acc: 98.50%
2025-10-29 11:59:14 - root - INFO - Running validation...
2025-10-29 12:02:56 - root - INFO - Learning rate reduced from 0.000091 to 0.000091
2025-10-29 12:02:56 - root - INFO - Checkpoint saved for epoch 34 at Baseline_B8/checkpoints/checkpoint_epoch_34.pth
2025-10-29 12:02:56 - root - INFO - Epoch 34/40 Results:
2025-10-29 12:02:56 - root - INFO -   Train Loss: 0.7594, Train Acc: 98.51%
2025-10-29 12:02:56 - root - INFO -   Val Loss: 0.9641, Val Acc: 88.44%
2025-10-29 12:02:56 - root - INFO -   Learning Rate: 0.000091
2025-10-29 12:02:56 - root - INFO - ------------------------------------------------------------
2025-10-29 12:02:56 - root - INFO - Starting epoch 35/40
2025-10-29 12:04:19 - root - INFO -   Batch 50/269 - Loss: 0.7227, Acc: 99.25%
2025-10-29 12:05:36 - root - INFO -   Batch 100/269 - Loss: 1.1880, Acc: 99.12%
2025-10-29 12:06:53 - root - INFO -   Batch 150/269 - Loss: 0.6850, Acc: 99.00%
2025-10-29 12:08:09 - root - INFO -   Batch 200/269 - Loss: 0.7093, Acc: 98.75%
2025-10-29 12:09:27 - root - INFO -   Batch 250/269 - Loss: 0.7185, Acc: 98.80%
2025-10-29 12:09:53 - root - INFO - Running validation...
2025-10-29 12:13:35 - root - INFO - Learning rate reduced from 0.000091 to 0.000091
2025-10-29 12:13:36 - root - INFO - Checkpoint saved for epoch 35 at Baseline_B8/checkpoints/checkpoint_epoch_35.pth
2025-10-29 12:13:36 - root - INFO - Epoch 35/40 Results:
2025-10-29 12:13:36 - root - INFO -   Train Loss: 0.7473, Train Acc: 98.79%
2025-10-29 12:13:36 - root - INFO -   Val Loss: 0.9717, Val Acc: 88.22%
2025-10-29 12:13:36 - root - INFO -   Learning Rate: 0.000091
2025-10-29 12:13:36 - root - INFO - ------------------------------------------------------------
2025-10-29 12:13:36 - root - INFO - Starting epoch 36/40
2025-10-29 12:15:00 - root - INFO -   Batch 50/269 - Loss: 0.8858, Acc: 98.00%
2025-10-29 12:16:15 - root - INFO -   Batch 100/269 - Loss: 0.7033, Acc: 98.50%
2025-10-29 12:17:32 - root - INFO -   Batch 150/269 - Loss: 1.2396, Acc: 98.67%
2025-10-29 12:18:48 - root - INFO -   Batch 200/269 - Loss: 0.6993, Acc: 98.75%
2025-10-29 12:20:04 - root - INFO -   Batch 250/269 - Loss: 0.6849, Acc: 98.80%
2025-10-29 12:20:33 - root - INFO - Running validation...
2025-10-29 12:24:24 - root - INFO - Learning rate reduced from 0.000091 to 0.000091
2025-10-29 12:24:24 - root - INFO - Checkpoint saved for epoch 36 at Baseline_B8/checkpoints/checkpoint_epoch_36.pth
2025-10-29 12:24:24 - root - INFO - Epoch 36/40 Results:
2025-10-29 12:24:24 - root - INFO -   Train Loss: 0.7472, Train Acc: 98.79%
2025-10-29 12:24:24 - root - INFO -   Val Loss: 0.9736, Val Acc: 88.14%
2025-10-29 12:24:24 - root - INFO -   Learning Rate: 0.000091
2025-10-29 12:24:24 - root - INFO - ------------------------------------------------------------
2025-10-29 12:24:24 - root - INFO - Starting epoch 37/40
2025-10-29 12:25:50 - root - INFO -   Batch 50/269 - Loss: 0.7212, Acc: 99.00%
2025-10-29 12:27:07 - root - INFO -   Batch 100/269 - Loss: 0.6943, Acc: 98.62%
2025-10-29 12:28:29 - root - INFO -   Batch 150/269 - Loss: 0.7053, Acc: 98.67%
2025-10-29 12:29:47 - root - INFO -   Batch 200/269 - Loss: 0.7592, Acc: 98.69%
2025-10-29 12:31:07 - root - INFO -   Batch 250/269 - Loss: 0.7171, Acc: 98.80%
2025-10-29 12:31:35 - root - INFO - Running validation...
2025-10-29 12:35:25 - root - INFO - Learning rate reduced from 0.000091 to 0.000091
2025-10-29 12:35:25 - root - INFO - Checkpoint saved for epoch 37 at Baseline_B8/checkpoints/checkpoint_epoch_37.pth
2025-10-29 12:35:25 - root - INFO - Epoch 37/40 Results:
2025-10-29 12:35:25 - root - INFO -   Train Loss: 0.7484, Train Acc: 98.75%
2025-10-29 12:35:25 - root - INFO -   Val Loss: 0.9729, Val Acc: 87.77%
2025-10-29 12:35:25 - root - INFO -   Learning Rate: 0.000091
2025-10-29 12:35:25 - root - INFO - ------------------------------------------------------------
2025-10-29 12:35:25 - root - INFO - Starting epoch 38/40
2025-10-29 12:36:52 - root - INFO -   Batch 50/269 - Loss: 0.8031, Acc: 98.75%
2025-10-29 12:38:09 - root - INFO -   Batch 100/269 - Loss: 0.7708, Acc: 98.88%
2025-10-29 12:39:26 - root - INFO -   Batch 150/269 - Loss: 0.7112, Acc: 99.08%
2025-10-29 12:40:44 - root - INFO -   Batch 200/269 - Loss: 0.6938, Acc: 99.06%
2025-10-29 12:42:02 - root - INFO -   Batch 250/269 - Loss: 0.7017, Acc: 98.95%
2025-10-29 12:42:28 - root - INFO - Running validation...
2025-10-29 12:46:13 - root - INFO - Learning rate reduced from 0.000091 to 0.000091
2025-10-29 12:46:14 - root - INFO - Checkpoint saved for epoch 38 at Baseline_B8/checkpoints/checkpoint_epoch_38.pth
2025-10-29 12:46:14 - root - INFO - Epoch 38/40 Results:
2025-10-29 12:46:14 - root - INFO -   Train Loss: 0.7414, Train Acc: 98.98%
2025-10-29 12:46:14 - root - INFO -   Val Loss: 0.9722, Val Acc: 87.47%
2025-10-29 12:46:14 - root - INFO -   Learning Rate: 0.000091
2025-10-29 12:46:14 - root - INFO - ------------------------------------------------------------
2025-10-29 12:46:14 - root - INFO - Starting epoch 39/40
2025-10-29 12:47:36 - root - INFO -   Batch 50/269 - Loss: 0.6906, Acc: 99.25%
2025-10-29 12:48:53 - root - INFO -   Batch 100/269 - Loss: 0.6917, Acc: 99.38%
2025-10-29 12:50:12 - root - INFO -   Batch 150/269 - Loss: 0.7631, Acc: 98.92%
2025-10-29 12:51:29 - root - INFO -   Batch 200/269 - Loss: 0.6842, Acc: 98.88%
2025-10-29 12:52:48 - root - INFO -   Batch 250/269 - Loss: 1.3464, Acc: 98.80%
2025-10-29 12:53:15 - root - INFO - Running validation...
2025-10-29 12:57:11 - root - INFO - Learning rate reduced from 0.000091 to 0.000091
2025-10-29 12:57:11 - root - INFO - Checkpoint saved for epoch 39 at Baseline_B8/checkpoints/checkpoint_epoch_39.pth
2025-10-29 12:57:11 - root - INFO - Epoch 39/40 Results:
2025-10-29 12:57:11 - root - INFO -   Train Loss: 0.7481, Train Acc: 98.88%
2025-10-29 12:57:11 - root - INFO -   Val Loss: 0.9696, Val Acc: 88.14%
2025-10-29 12:57:11 - root - INFO -   Learning Rate: 0.000091
2025-10-29 12:57:11 - root - INFO - ------------------------------------------------------------
2025-10-29 12:57:11 - root - INFO - Starting epoch 40/40
2025-10-29 12:58:37 - root - INFO -   Batch 50/269 - Loss: 0.7300, Acc: 99.25%
2025-10-29 12:59:59 - root - INFO -   Batch 100/269 - Loss: 0.7747, Acc: 99.38%
2025-10-29 13:01:20 - root - INFO -   Batch 150/269 - Loss: 0.8025, Acc: 99.25%
2025-10-29 13:02:41 - root - INFO -   Batch 200/269 - Loss: 0.7380, Acc: 99.06%
2025-10-29 13:04:02 - root - INFO -   Batch 250/269 - Loss: 0.6913, Acc: 99.15%
2025-10-29 13:04:30 - root - INFO - Running validation...
2025-10-29 13:08:18 - root - INFO - Learning rate reduced from 0.000091 to 0.000091
2025-10-29 13:08:18 - root - INFO - Checkpoint saved for epoch 40 at Baseline_B8/checkpoints/checkpoint_epoch_40.pth
2025-10-29 13:08:18 - root - INFO - Epoch 40/40 Results:
2025-10-29 13:08:18 - root - INFO -   Train Loss: 0.7397, Train Acc: 99.12%
2025-10-29 13:08:18 - root - INFO -   Val Loss: 0.9869, Val Acc: 87.99%
2025-10-29 13:08:18 - root - INFO -   Learning Rate: 0.000091
2025-10-29 13:08:18 - root - INFO - ------------------------------------------------------------
2025-10-29 13:08:18 - root - INFO - Training completed!
2025-10-29 13:08:18 - root - INFO - Best validation loss achieved: 0.9511
2025-10-29 13:08:18 - root - INFO - Loading best model for final evaluation...
2025-10-29 13:12:03 - root - INFO - Final validation accuracy: 88.52%
2025-10-29 13:12:03 - root - INFO - training completed successfully
2025-10-29 13:12:05 - numexpr.utils - INFO - NumExpr defaulting to 4 threads.
